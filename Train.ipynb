{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imported libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 568454 entries, 0 to 568453\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   ProductId  568454 non-null  object\n",
      " 1   Text       568454 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 8.7+ MB\n",
      "None\n",
      "    ProductId                                               Text\n",
      "0  B001E4KFG0  I have bought several of the Vitality canned d...\n",
      "1  B00813GRG4  Product arrived labeled as Jumbo Salted Peanut...\n",
      "2  B000LQOCH0  This is a confection that has been around a fe...\n",
      "3  B000UA0QIQ  If you are looking for the secret ingredient i...\n",
      "4  B006K2ZZ7K  Great taffy at a great price.  There was a wid...\n",
      "5  B006K2ZZ7K  I got a wild hair for taffy and ordered this f...\n",
      "6  B006K2ZZ7K  This saltwater taffy had great flavors and was...\n",
      "7  B006K2ZZ7K  This taffy is so good.  It is very soft and ch...\n",
      "8  B000E7L2R4  Right now I'm mostly just sprouting this so my...\n",
      "9  B00171APVA  This is a very healthy dog food. Good for thei...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "importData = pd.read_csv('Reviews.csv')\n",
    "data = importData[['ProductId', 'Text']]\n",
    "print(data.info())\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performed text preprocessing for each text review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words tokenized by product ID: \n",
      "B001E4KFG0: ['buy', 'sever', 'vital', 'can', 'dog', 'food', 'product', 'find', 'good', 'qualiti', '', 'product', 'look', 'like', 'stew', 'process', 'meat', 'smell', 'better', '', 'labrador', 'finicki', 'appreci', 'product', 'better', '']\n",
      "B00813GRG4: ['product', 'arriv', 'label', 'jumbo', 'salt', 'peanut', '', 'peanut', 'actual', 'small', 'size', 'unsalt', '', 'sure', 'error', 'vendor', 'intend', 'repres', 'product', '', 'jumbo', '', '']\n",
      "B000LQOCH0: ['confect', 'around', 'centuri', '', 'light', '', 'pillowi', 'citru', 'gelatin', 'nut', '', 'case', 'filbert', '', 'cut', 'tini', 'squar', 'liber', 'coat', 'powder', 'sugar', '', 'tini', 'mouth', 'heaven', '', 'chewi', '', 'flavor', '', 'highli', 'recommend', 'yummi', 'treat', '', 'familiar', 'stori', 'cs', '', 'lewi', '', '', 'lion', '', 'witch', '', 'wardrob', '', '', 'treat', 'seduc', 'edmund', 'sell', 'brother', 'sister', 'witch', '']\n",
      "B000UA0QIQ: ['look', 'secret', 'ingredi', 'robitussin', 'believ', 'find', '', 'get', 'addit', 'root', 'beer', 'extract', 'order', '', 'good', '', 'make', 'cherri', 'soda', '', 'flavor', 'medicin', '']\n",
      "B006K2ZZ7K: ['great', 'taffi', 'great', 'price', '', 'wide', 'assort', 'yummi', 'taffi', '', 'deliveri', 'quick', '', 'taffi', 'lover', '', 'deal', '', 'get', 'wild', 'hair', 'taffi', 'order', 'five', 'pound', 'bag', '', 'taffi', 'enjoy', 'mani', 'flavor', '', 'watermelon', '', 'root', 'beer', '', 'melon', '', 'peppermint', '', 'grape', '', 'etc', '', 'complaint', 'bite', 'much', 'redblack', 'licoriceflavor', 'piec', '', 'particular', 'favorit', '', '', '', 'kid', '', 'husband', '', 'last', 'two', 'week', '', 'would', 'recommend', 'brand', 'taffi', '', 'delight', 'treat', '', 'saltwat', 'taffi', 'great', 'flavor', 'soft', 'chewi', '', 'candi', 'individu', 'wrap', 'well', '', 'none', 'candi', 'stick', 'togeth', '', 'happen', 'expens', 'version', '', 'fraling', 's', '', 'would', 'highli', 'recommend', 'candi', '', 'serv', 'beachthem', 'parti', 'everyon', 'love', '', 'taffi', 'good', '', 'soft', 'chewi', '', 'flavor', 'amaz', '', 'would', 'definit', 'recommend', 'buy', '', 'satisfi', '', '']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "cleanedData = {}\n",
    "\n",
    "#Word tokenization\n",
    "for index, row in data.iterrows() :\n",
    "    tokens = word_tokenize(row['Text'])\n",
    "    id = row['ProductId']\n",
    "    if id in cleanedData.keys() :\n",
    "        cleanedData[id].append(tokens)\n",
    "    else :\n",
    "        cleanedData[id] = [tokens]\n",
    "\n",
    "for product, tokens in cleanedData.items():\n",
    "\n",
    "    # Lowercasing\n",
    "    lowercased_tokens = [word.lower() for sublist in tokens for word in sublist]\n",
    "\n",
    "    # Removing stopwords\n",
    "    stops = set(stopwords.words('english'))\n",
    "    without_stopwords = [word for word in lowercased_tokens if word not in stops]\n",
    "\n",
    "    # Removing punctuation\n",
    "    without_punctuation = [re.sub(r'\\W+', '', word) for word in without_stopwords]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in without_punctuation]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word, pos='v') for word in stemmed_tokens]\n",
    "\n",
    "    # Assign the final list to cleanedData[product]\n",
    "    cleanedData[product] = lemmatized_tokens\n",
    "\n",
    "print(\"Words tokenized by product ID: \")\n",
    "for key, value in list(cleanedData.items())[:5] :\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented NLP model: Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '0000' ... 'être' 'île' 'ît']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(data['Text'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for x in range(0, len(data['Text']), 1000) :\n",
    "    X = vectorizer.transform(data['Text'][x:x+1000])\n",
    "print(feature_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
